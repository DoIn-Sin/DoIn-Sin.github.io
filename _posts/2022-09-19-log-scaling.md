---
title: 로그 스케일링 톺아보기
description: 로그 스케일링 알고 쓰자!
categories:
- data analysis
- statistics
tags: 
- log scailing
- statistics
---

# 로그 스케일링이 뭐냐고..
데이터 분석이나 머신러닝 공부를 하면 꼭 나오는 로그 스케일링..뭐길래 할 때도 있고 안 할때도 있는걸까?
대부분의 책에는 정규화를 위해 한다고 하는데 왜 정규화가 되는건지 정규화가 되면 뭐가 좋길래 정규화를 하는건지 알아보자.

## 정규화(Normalization)의 필요성
로그 스케일링을 알기 전에 정규화가 왜 필요한지부터 알아야 한다. 우리가 다루는 머신러닝 알고리즘들은 데이터가 가진 feature들을 비교하여 데이터의 패턴을 찾는다.

그런데 만약 feature의 스케일이 심하게 차이가 나면 어떻게 될까? 어느 한 쪽으로 편향되기 때문에 예측 결과에 대한 신뢰성이 많이 떨어질 것이다.

따라서 우리는 차이가 많이 나는 경우(=데이터가 어느 한 쪽으로 편향된 경우, 데이터의 스케일 차이가 심한 경우)를 최대한 줄이기 위해 정규화를 하는 것이다!

## 로그 스케일링
로그 변환은 보통 큰 수를 작게 만들 경우, 복잡한 계산을 간편하게 만들 경우에 사용한다. 로그를 취하는 순산 그 수는 진수가 되어버리니 값이 작아지고, 로그의 성질에 따라 곱셈은 진수의 덧셈으로, 나누기는 뺄셈으로 바뀌면서 계산이 간단해진다! 특히, 데이터 간 간격이 클 경우 유용하게 작용된다!!

>예시 1) 큰 수를 작게 만들 경우
100 = 10^2이다. 여기에 상용로그를 취한다면, 100을 10을 밑으로 하는 지수가 있는 값의 그 지수를 나타낸다. 즉, 2가 된다는 말이다.

>예시 2) 복잡한 계산을 간편하게 만드는 경우
log10에 로그를 취하면 로그의 성질에 의해 곱하기가 더하기로, 나누기가 빼기로 바뀐다.

>예시 3) 로그 변환이 꼭 정규분포 변환에 한정되지 않음을 보여주는 예시
![image](https://user-images.githubusercontent.com/77676907/190958857-f9edf106-c76f-4479-9f82-764c598a0bf8.png)
1960년대의 큰 차이를 보는 값들은 log로 인해 그 차이의 스케일이 작아지고 (ex 제곱단위의 정수 변화) 1970~2010의 값도 log변환으로 스케일이 작아졌지만 60년 값에 비해 그 폭이 작다. (ex 제곱 안 수의 소수자리 변화) <br>
따라서 전체 스케일이 평준화되도록 각 값들이 작아졌기 때문에 분석이 용이해졌다. 

## 그럼 로그 스케일링이 짱이네?
결론적으로 그렇지는 않다! 로그 스케일링도 결국 원본 데이터를 변환(=성질변환)시키는 것이기 때문에 로그 변환 후 원본 데이터와 상관성이 떨어지는 경우가 있기 때문이다.

### 뭐야 그럼 안 쓰는게 더 낫나..?
이건 또 틀린 말이다..ㅎ 우리는 지금까지 전통적으로 값의 크기를 맞출 때, 계산을 편리하게 하기 위해 로그 변환을 종종 사용해왔다. 또한, 위에서 언급한 문제도 해결가능한 부분이 있는 것이, 변환을 하더라도 원본 데이터의 성질보존력이 뛰어나기 때문에 현재도 많이 사용하고 있다!

어느 정도 안정성이 확보된 기법이기 때문에 아무 생각없이 사용하지만 않는다면 무방하다고 판단한다!

(여담) 간혹 '변수가 로그정규분포이기 때문에 로그변환을 해서 정규분포로 만들어준다'고 하기도 하는데, 적절한 답은 아니라고 생각한다. 변수가 로그정규분포에서 나오든, 카이제곱분포에서 나오든 가설검정 단계에서 중요한 건 추정량의 분포이지, 변수의 분포가 아니다. 로그변환을 함으로써 원래는 로그정규분포인 '추정량'을 정규분포로 만들어준다는 이야기라면 모를까, '변수'의 로그정규분포 여부는 그리 중요하지는 않다. 적어도 회귀분석을 할 때는.